\documentclass[12pt,a4paper,twocolumn]{article}
\usepackage[margin=2cm,columnsep=0.8cm]{geometry} 
\usepackage{setspace}                    
\usepackage{authblk}                      
\usepackage{graphicx}                    
\usepackage{subcaption}                 
\usepackage{float}                       
\usepackage{booktabs}                   
\usepackage{amsmath,amsfonts,amssymb}   
\usepackage{siunitx}                  
\usepackage[round,sort&compress]{natbib}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{breakurl}
\urlstyle{same}   
\usepackage{ragged2e}
\justifying               
\usepackage{caption}                    
\usepackage{fancyhdr}                   
\usepackage{xcolor}                    
\usepackage{libertine}
\setlength{\parskip}{1.5ex}
\setlength{\parindent}{0pt}
\captionsetup{font=small,labelfont=bf}
\renewcommand{\bibfont}{\justifying} 
\usepackage{etoolbox}
\apptocmd{\thebibliography}{\justifying}{}{%
  \GenericError{}{Patching \thebibliography failed}{}{}
}


\fancyhf{}
\fancyhead[L]{\textit{Did they lie? Fake news detection in NLP}}  % Edit the running title (short title)
\fancyhead[R]{\thepage}

% For incoming students, feel free to use your home university here
\title{My NLP Project LUL} % Pick your title (you may go creative here)
\author[1]{Braunegg, Florian} % Each author should be listed
\author[2]{Second Author}
\affil[1]{Graz University of Technology, Graz, Austria}
\affil[2]{University of Graz, Graz, Austria}
\date{\today}
\makeatletter
\g@addto@macro\UrlBreaks{\do\/\do-\do.\do?\do=\do&\do_\do:\do\#\do\%\do\\}
\makeatother
\begin{document}
\maketitle


\begin{abstract}
    Your abstract goes here. It should be a single paragraph of 150--250 words summarizing the report’s context, aims, methods, results, and conclusions.
    The entire paper should not exceed 6 pages (excluding limitations, ethical considerations, references, appendix).
    Please do not forget to fill out the contributions for each team member in the table.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Fake news have become a major concern in contemporary political and social discourse. They contribute to the polarization and fragmentation of society \citep{au2022role} and, in extreme cases, may even pose a threat to public safety. This is exemplified by incidents such as Pizzagate, in which an armed man stormed a pizzeria in Washington, D.C., attempting to rescue non-existent children based on fabricated information \citep{2016pizzagate}. Due to their wide-ranging impact, addressing fake news effectively requires an approach that integrates both social and technical perspectives. While disciplines like psychology and sociology ofte explore how such misinformation spreads and shapes public opinion, computational fields, especially Natural Language Processing (NLP), provide tools to systematically analyze and detect such content. NLP enables the identification of linguistic patterns and contextual signals that are typical of deceptive or false information. Consequently, the reliable detection and classification of fake news remains a core challenge that NLP is uniquely equipped to address.

As part of the course ``Natural Language Processing'' at the Graz University of Technology, we analyzed and classified fake news in three consecutive stages that illustrate NLP techniques for fake news detection.

\section{Related Work}
Fake news can be broadly understood as information presented in the form of news that is intentionally false and designed to mislead its audience. While the definition is not universally agreed upon, broader interpretations such as the one adopted in this paper treat fake news as an umbrella term that includes both misinformation (false information shared without intent to deceive) and disinformation (false information shared deliberately). In this broader perspective, any misleading or incorrect content presented as news, regardless of intent, may be classified as fake news (\citeauthor{deOliveira2021} \citeyear{deOliveira2021}; for a discussion of the terminology see \citeauthor{article} \citeyear{article}). Although instances of fake news can be traced as far back as 2100–1200 BC, as seen in the Babylonian Epic of Gilgamesh \citep{Roozenbeek2024Psychology}, scholarly interest in the phenomenon has only gained significant momentum in recent years \citep{info15120742}. Despite definitional differences, fake news have far-reaching consequences for social cohesion, public opinion, institutional trust, and political development. Prominent examples of these consequences include the election of U.S. President Donald Trump \citep{allcott2017social}, the Brexit referendum \citep{orlando2023posttruth}, and the COVID-19 crisis \citep{ferreira2022impact}, all of which are closely linked to the digitization of society.

Interest in fake news detection within the field of NLP began to grow significantly in the second decade of the 21st century. Early research on fake news detection primarily relied on traditional machine learning algorithms and, in some cases, rule-based systems. These systems operate by defining sets of rules or linguistic heuristics to classify articles. Rule-based systems generally exhibit lower accuracy compared to more advanced approaches, primarily due to their limited ability to capture contextual dependencies and adapt to dynamic linguistic variations. Consequently, they are prone to generating a high rate of false positives, especially when applied to sophisticated fake narratives or texts that deviate from their predefined rule configurations \citep{polu2024ai, repede2023comparison}. Applications of such rule-based approaches can be found, for example, in \citet{alotaibi2022rule}, who examined the spread of Arabic fake news during the COVID-19 pandemic, and in \citet{yuliani2019framework}, who developed a rule-based framework for hoax detection. When applied to larger or more complex text corpora, traditional machine learning algorithms such as Logistic Regression or Random Forrest often outperform rule-based systems. Traditional machine learning algorithms rely on statistical modeling techniques and are capable of learning patterns directly from data. Although traditional machine learning algorithms are more flexible than rule-based systems, they still lack the ability to capture contextual information within text and depend heavily on manually engineered features and expert domain knowledge. Moreover, large amounts of data are often required to effectively train and fine-tune these algorithms for specific tasks. Consequently, the performance and applicability of such models are highly dependent on how the problem is defined and on the quality of the manually engineered features they utilize \citep{polu2024ai, pittman2025truthtextmetaanalysismlbased}. Despite these limitations, comparative studies such as that by \citet{Sudhakar2022Prediction} demonstrate the considerable potential of traditional machine learning methods. The advent of deep learning marked a significant advancement in computational classification tasks, with models such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks (LSTMs) capable of learning hierarchical and sequential patterns directly from data. However, the introduction of the transformer architecture and the subsequent development of models such as BERT, RoBERTa, and GPT represented a major breakthrough in natural language processing. Unlike earlier architectures, transformers leverage contextual embeddings and attention mechanisms to capture subtle dependencies and nuanced patterns in text that traditional machine learning models cannot handle. However, deep learning approaches still require substantial amounts of data, annotated datasets, and significant computational resources in order to achieve strong performance. Another major limitation of deep learning models is their lack of interpretability, as they often function as black-box systems whose internal decision-making processes are difficult to trace or explain \citep{polu2024ai, pittman2025truthtextmetaanalysismlbased}. In response to the limitations of individual approaches, recent research has proposed hybrid models for fake news detection that aim to combine the strengths of different techniques. \citet{Nasir2021Hybrid} for example proposed a CNN-RNN hybrid model and \citet{Albahar2021Hybrid} an SVM-RNN-BI-GT hybrid model for fake news detection.

\section{Materials and Methods}
\label{sec:methods}
Our dataset consists of 150 articles of both fake and real news, collected and provided by students and faculty at Graz University of Technology. For stages two and three, we implemented all code using Python. In the first stage, we manually evaluated all articles to create a labeled dataset for the subsequent stages and to gain an initial understanding of how fake news is written. In doing so, we followed the definition of fake news outlined earlier in the related work section.

In the second stage, we imported the dataset using pandas \citep{Pandas} and employed the Random Forest classifier from scikit-learn \citep{scikit-learn} to classify the 150 articles as either fake or real. As features for our classifier, we extracted part-of-speech (PoS) tags, named entities (used in combination with a knowledge base), emotional content in the text, readability and difficulty scores, as well as grammatical and spelling errors. Each feature was extracted separately for both the headline and the body text of each article. We selected these features for their straightforward and interpretable nature, in contrast to statistical representations like n-grams or TF-IDF.

PoS-tags were extracted using spaCy \citep{spaCy}, which also handled the tokenization. We restricted the PoS-tags to the 17 universal part-of-speech categories defined in the Universal Dependencies framework \citep{universaldependencies} to extract easily interpretable logical units. The frequency of each tag was calculated relative to the total number of tokens in the article.

The knowledge base, based on fake claims that were extracted with a one-shot prompt from GPT-4o mini to avoid bias towards our articles and due to time constraints, consists of two elements: entities (with aliases) and concise fake claims. Both elements were tokenized with spaCy and lowercased to improve matching. The claims were additionally lemmatized and filtered using spaCy’s stopword list. Entities and aliases were searched in the text using spaCy’s PhraseMatcher. For every match, we looked for the corresponding claims in a list of all sentences from the article that contained the entity. These sentences were lemmatized, lowercased, and filtered for alphanumeric characters and stopwords. If multiple entities were found in a sentence, each was checked separately. Since it was expected that not every claim would be worded exactly as in the knowledge base, we used WordNet provided by NLTK \citep{NLTK} to extract synonyms. These synonyms were also lowercased and lemmatized to improve matching accuracy. A match was recorded as soon as two words, representing the smallest logical unit, were found in the same sentence. As a feature, we extracted the number of mismatches relative to the number of sentences in the articles.

To detect emotions in the text, we compared all tokens extracted with spaCy against an extended version of the NRC Word–Emotion Association Lexicon \citep{emotions}, as provided by \citet{NRClex}. We calculated the relative frequency of each emotion in proportion to the total number of emotional terms in the article. Based on our findings during the manual analysis in stage one, we focused on four prevalent emotions: anger, anticipation, fear, and sadness.

To further assess article readability, we used the Flesch Reading Ease \citep{flesch} and the Automated Readability Index \citep{senter_smith_1967}. The Flesch Reading Ease score ranges from 100, indicating very easy to read, to 0, indicating very difficult to read. The score decreases with longer average sentence lengths and more syllables per word, as longer constructions are considered harder to process. The Automated Readability Index usually ranges from 1 to 14 or higher, corresponding to U.S. school grade levels, and is based on the number of words per sentence and characters per word. To estimate word difficulty, we computed the proportion of difficult words relative to all words in the article, using the word list from \citet{chall1995readability}. Both the readability score and the difficulty measure were calculated using textstat \citep{textstat}.

Grammatical and spelling errors were detected using LanguageTool for Python \citep{ltp}. We calculated the number of detected issues relative to the total word count as an additional feature.


To determine the optimal combination of features and hyperparameters and to evaluate model performance, we split the dataset into an 80/20 train-test split. Hyperparameter optimization was performed using GridSearchCV from scikit-learn, with accuracy as the target metric and RepeatedStratifiedKFold with five folds and three repeats as the cross-validation strategy. This approach systematically tests various parameter combinations while controlling for overfitting. To refine the feature set, we applied the Boruta algorithm \citep{boruta1}, implemented in Python by \citet{boruta2}. This method evaluates feature importance by comparing actual features to randomly permuted shadow features. We selected features that ranked above the 90th percentile in importance scores, with statistical significance assessed at the 5\% level. As a final step, model performance was evaluated using repeated hold-out validation over 100 random splits, reporting accuracy, macro-averaged precision, recall, and F1-score.


\section{Results}
\label{sec:results}
Present the evaluation results you gained (objective results). 
Consider using tables and charts for the report.
Of course, baselines are helpful for the reader to assess the performance of the method.

\section{Discussion}
\label{sec:discussion}
Interpret and discuss your results (could be a more subjective results).
Describe your findings here, what can we learn from the work.

\section{Conclusion}
\label{sec:conclusion}
Summarize the key findings and implications (design your report that one get the main insights from reading abstract/introduction/conclusions and glancing at the illustrations). 
Suggest future research (very briefly).


% Everything up here counts for the page limit 
\newpage

\section*{Limitations}
List potential limitations of your work, e.g., only English language.

\section*{Ethical Considerations}
Please consider how your work could potentially be used and may cause some harm.
Also, sustainability aspects can be reported here, e.g., how much CO2e does you approach require.

\section*{Acknowledgments}
Briefly acknowledge people, funding sources, or institutions.

\section*{Contributions}
% Add here a list of the individual contributions per team member
\begin{table}[h!]
    \centering
    \caption{List of contributions per team member.}
    \label{tab:contributions}
    \begin{tabular}{lp{4cm}}
        \toprule
        \textbf{Team Member} & \textbf{Contribution} \\
        \midrule
        Florian Braunegg           & Stage0, Stage1, Stage2, Introduction, Relate Work, Methods \& Materials (Materials, Stage1, Stage2), Results(Stage1, Stage2)             \\
        Another Name    & Did not even care to show up.    \\
        \bottomrule
    \end{tabular}
\end{table}

\bibliographystyle{plainnat}
\bibliography{bibliography}

\appendix

\section{Appendix - Overview}

The appendix can be used to add details, especially implementation aspects, or added evaluations.
There is no page limit on the appendix.
You may also report approaches that you tried, but did not work out.
Additional examples can be reported, or prompts being used for generative AI.

\section{Appendix - Usage of AI}
More details on the usage of AI: \href{https://www.tugraz.at/fileadmin/Studierende\_und\_Bedienstete/Information/Unsere\_TU\_Graz/Lehre\_an\_der\_TU\_Graz/Zitiervorschlaege_KI.pdf}{Zitiervorschlaege AI}

\section{Appendix - Figure and Table Examples}
Examples how to use figures, see Figure~\ref{fig:example} and tables, see Table~\ref{tab:contributions}.
In double, just let Latex layout the illustrations for you, or position them at the top or bottom of a page.
Is is common to capitalise nouns that are followed by numbers, as they are considered names (proper noun), e.g., Page 4.

\begin{figure}
    \centering
        \includegraphics[width=0.7\linewidth]{example-image}
    \caption{Example figure caption. Please consider to explain to the reader, what is depicted}
    \label{fig:example}
\end{figure}


\end{document}
