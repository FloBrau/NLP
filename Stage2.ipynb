{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea666b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas spacy textstat language-tool-python numpy scikit-learn boruta\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b06293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import textstat\n",
    "import language_tool_python\n",
    "import numpy as np\n",
    "import random\n",
    "from spacy.tokens import Doc\n",
    "from urllib.request import urlopen\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from boruta import BorutaPy\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "language_tool = language_tool_python.LanguageTool(\"en-US\")\n",
    "#loading the nrc_en lexicon manually as the NRCLex 4.0 is very buggy\n",
    "url = \"https://raw.githubusercontent.com/DemetersSon83/NRCLex/refs/heads/master/nrc_en.json\" \n",
    "with urlopen(url) as re:\n",
    "    lexicon = json.load(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599ddc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles (filepath: str) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Load the articles as a pandas DF\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    article = []\n",
    "    with open(filepath, \"r\", encoding = \"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            article.append(json.loads(line))\n",
    "\n",
    "    return pd.DataFrame(article)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b245580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts (articles: list[str]) -> list[Doc]:\n",
    "\n",
    "    \"\"\"\n",
    "        Use \"en_core_web_sm\" model on df[\"Text\"] and df[\"Title]\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    return list(nlp.pipe(articles))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def POStagging (articles: list[Doc], df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Counting and processing POS-Tags\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #filter POS-tags we want\n",
    "    #we stick with universal POS-tags for now\n",
    "    #https://github.com/explosion/spaCy/blob/master/spacy/glossary.py\n",
    "    universal_tags = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\",\n",
    "                      \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "    \n",
    "    \n",
    "    #retrieving the IDs of the relevant POS-tags\n",
    "    tag_ids = {tag: nlp.vocab.strings[tag] for tag in universal_tags}\n",
    "\n",
    "\n",
    "    temp_df_list = []\n",
    "    for article in articles:\n",
    "\n",
    "\n",
    "        #count all tokens (without space-tokens)\n",
    "        total_tokens = sum(1 for token in article if not token.is_space)\n",
    "\n",
    "\n",
    "        #counting all POS-tags\n",
    "        pos_counts = article.count_by(spacy.attrs.POS)\n",
    "\n",
    "\n",
    "        #adding the POS-Tags as percentage of the total tokens + total count \n",
    "        inner_temp = {}\n",
    "        for tag, tag_id in tag_ids.items():\n",
    "            counter = pos_counts[tag_id] if tag_id in pos_counts else 0\n",
    "            inner_temp[f\"{tag.lower()}_count\"] = counter\n",
    "            inner_temp[f\"{tag.lower()}_perc\"] = counter / total_tokens if total_tokens else 0 \n",
    "        temp_df_list.append(inner_temp)  \n",
    "\n",
    "\n",
    "    #create a pd DF with the same index as df\n",
    "    df_features = pd.DataFrame(temp_df_list, index = df.index)\n",
    "    return pd.concat([df,df_features], axis = 1)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NER (articles: list[Doc], df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "    \"\"\"\n",
    "        Counting and processing NER\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #many random forrest iterations showed, that these are the important one\n",
    "    ner_labels = [\"PERSON\", \"ORG\", \"GPE\", \"DATE\", \"NORP\", \"CARDINAL\", \n",
    "                  \"MONEY\", \"PERCENT\", \"LOC\", \"EVENT\"]      \n",
    "\n",
    "\n",
    "    temp_df_list = []\n",
    "    for article in articles:\n",
    "\n",
    "\n",
    "        #set all labels to a default value\n",
    "        inner_temp = {}\n",
    "        for label in ner_labels:\n",
    "           inner_temp[f\"{label.lower()}\"] = []\n",
    "           inner_temp[f\"{label.lower()}_count\"] = 0\n",
    "           inner_temp[f\"{label.lower()}_unique\"] = 0\n",
    "\n",
    "\n",
    "        #retrieving the entities for the NER \n",
    "        entities = {label: [] for label in ner_labels}\n",
    "        for entity in article.ents:\n",
    "            if entity.label_ in ner_labels:\n",
    "                entities[entity.label_].append(entity.text)\n",
    "\n",
    "        \n",
    "        #extracting the raw text and count of the (unique) words\n",
    "        for key, value in entities.items():\n",
    "            inner_temp[f\"{key.lower()}\"] = value \n",
    "            inner_temp[f\"{key.lower()}_count\"] = len(value) \n",
    "            inner_temp[f\"{key.lower()}_unique\"] = len(set(value)) \n",
    "        temp_df_list.append(inner_temp)    \n",
    "\n",
    "\n",
    "    #create a pd DF with the same index as df\n",
    "    df_features = pd.DataFrame(temp_df_list, index = df.index)\n",
    "    return pd.concat([df,df_features], axis = 1)  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def emotions_nrc (articles: list[Doc], df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Counting and processing the emotions in the articles \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #filtering all emotions tags   \n",
    "    emotion_tags = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\",\n",
    "                    \"trust\", \"positive\", \"negative\"]\n",
    "    \n",
    "\n",
    "    #the following code is inspired by the NRCLex module by metalcorebear:\n",
    "    #i only recreate parts of the code as the module itselfe tends to cause problems \n",
    "    #source: https://pypi.org/project/NRCLex/\n",
    "\n",
    "\n",
    "    lex_keys = set(lexicon.keys())\n",
    "    temp_df_list = []\n",
    "    for article in articles:\n",
    "\n",
    "\n",
    "        #filtering and processing all relevant tokens\n",
    "        affect_list = []\n",
    "        for tok in article:\n",
    "            if not tok.is_alpha:\n",
    "                continue\n",
    "            orig = tok.lower_\n",
    "            if orig in lex_keys:    \n",
    "                affect_list.extend(lexicon[orig])    \n",
    "\n",
    "\n",
    "        #creating relevant counters\n",
    "        freq_counter = Counter()\n",
    "        for emo in affect_list:\n",
    "            freq_counter[emo] += 1\n",
    "        total_emotions = sum(freq_counter.values()) or 1        \n",
    "\n",
    "\n",
    "        #adding the emotions as percentage of the total emotinos + total count \n",
    "        inner_temp = {}\n",
    "        for emotion in emotion_tags:\n",
    "            emotion_counter = freq_counter[emotion] if emotion in freq_counter else 0\n",
    "            inner_temp[f\"{emotion.lower()}_count\"] = emotion_counter\n",
    "            inner_temp[f\"{emotion.lower()}_perc\"] = emotion_counter / total_emotions\n",
    "        temp_df_list.append(inner_temp)    \n",
    "\n",
    "\n",
    "    #create a pd DF with the same index as df\n",
    "    df_features = pd.DataFrame(temp_df_list, index = df.index)\n",
    "    return pd.concat([df,df_features], axis = 1)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readability_and_difficulty_scores (articles: list[str], df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Counting and processing readability scores and difficult worrds\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    temp_df_list = []\n",
    "\n",
    "    for article in articles:\n",
    "        difficult_words = textstat.difficult_words(article)\n",
    "        inner_temp = {\n",
    "\n",
    "            #different difficulties\n",
    "            #https://github.com/kupolak/textstat/blob/master/README.md#spache-readability-formula (for scales)\n",
    "            \"flesch_reading_ease\": textstat.flesch_reading_ease(article),\n",
    "            \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(article),\n",
    "            \"gunning_fog\": textstat.gunning_fog(article),\n",
    "            \"auto_readability_index\": textstat.automated_readability_index(article),\n",
    "            \"coleman_liau_index\": textstat.coleman_liau_index(article),\n",
    "            \"linsear_write_formula\": textstat.linsear_write_formula(article),\n",
    "\n",
    "            #difficult words count + perc of total words\n",
    "            \"difficult_words_count\": difficult_words,\n",
    "            \"difficult_words_perc\": difficult_words / textstat.lexicon_count(article,\n",
    "                                                                             removepunct = True)\n",
    "\n",
    "        }\n",
    "        temp_df_list.append(inner_temp)\n",
    "\n",
    "\n",
    "    #create a pd DF with the same index as df\n",
    "    df_features = pd.DataFrame(temp_df_list, index = df.index)\n",
    "    return pd.concat([df,df_features], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Error_check (articles: list[str], df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        Counting and processing the grammar and spelling check\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    temp_df_list = []\n",
    "    for article in articles:\n",
    "\n",
    "        matches = language_tool.check(article)\n",
    "\n",
    "        only_s = [entry for entry in matches if entry.ruleIssueType == \"misspelling\"]    \n",
    "        only_g = [entry for entry in matches if entry.ruleIssueType == \"grammar\"]\n",
    "        total_s = len(only_s)\n",
    "        total_g = len(only_g)  \n",
    "        total_words = textstat.lexicon_count(article, removepunct = True)\n",
    "\n",
    "\n",
    "        inner_temp = {\n",
    "\n",
    "            \"misspellings_count\": total_s,\n",
    "            \"misspellings_perc\": total_s / total_words,\n",
    "            \"grammar_count\": total_g,\n",
    "            \"grammar_perc\": total_g / total_words\n",
    "\n",
    "        }\n",
    "        temp_df_list.append(inner_temp)    \n",
    "\n",
    "\n",
    "\n",
    "    #create a pd DF with the same index as df\n",
    "    df_features = pd.DataFrame(temp_df_list, index = df.index)\n",
    "    return pd.concat([df,df_features], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11169e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df (df: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Connect the text and headline df\n",
    "    \"\"\"\n",
    "\n",
    "    #change column names for the headlines\n",
    "    df2_copy = df2.copy()\n",
    "    df2_copy.columns = [\"hl_\" + col for col in df2_copy.columns]\n",
    "\n",
    "    return pd.concat([df, df2_copy], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26333732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load\n",
    "file_path = Path(\"C:/Users/Admin/Desktop/Uni/CSS/NLP/without_assessment.jsonl\")\n",
    "df = load_articles(str(file_path))\n",
    "df2 = df.copy()\n",
    "\n",
    "\n",
    "#Text\n",
    "\n",
    "\n",
    "#prepare text\n",
    "texts = df[\"Text\"].tolist()\n",
    "docs  = process_texts(texts)\n",
    "\n",
    "\n",
    "#POS- and NERfeat\n",
    "df = POStagging(docs, df)\n",
    "df = NER(docs, df)\n",
    "\n",
    "\n",
    "#add emotions\n",
    "df = emotions_nrc(docs, df)\n",
    "\n",
    "\n",
    "#readability and difficulty\n",
    "df = readability_and_difficulty_scores(texts, df)\n",
    "\n",
    "\n",
    "#grammar and spelling errors\n",
    "df = Error_check(texts, df)\n",
    "\n",
    "\n",
    "#Headline\n",
    "\n",
    "\n",
    "#prepare text\n",
    "titles = df2[\"Title\"].tolist()\n",
    "docs  = process_texts(titles)\n",
    "\n",
    "\n",
    "#POS- and NERfeat\n",
    "df2 = POStagging(docs, df2)\n",
    "df2 = NER(docs, df2)\n",
    "\n",
    "\n",
    "#add emotions\n",
    "df2 = emotions_nrc(docs, df2)\n",
    "\n",
    "\n",
    "#readability and difficulty\n",
    "df2 = readability_and_difficulty_scores(titles, df2)\n",
    "\n",
    "\n",
    "#grammar and spelling errors\n",
    "df2 = Error_check(titles, df2)\n",
    "\n",
    "\n",
    "df = concat_df(df, df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"output_features.csv\", index = True, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40946f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation (df: pd.DataFrame, csv_path_labels: str, labelname: str) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" \n",
    "        pereparing the df for the ML part\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    df = df.drop(columns = [\"Unnamed: 0\", \"Index\", \"hl_Index\"]) \n",
    "    df.drop(columns=[c for c in df.columns if \"count\" in c.lower()], inplace=True)\n",
    "    \n",
    "       \n",
    "    #filtering the df for only ints and floats\n",
    "    df_filtered = df.select_dtypes(include = [\"int64\", \"float64\"]).copy()\n",
    "\n",
    "\n",
    "    #filtering all columns that have little variance \n",
    "    n = len(df_filtered)\n",
    "    low_variance = []\n",
    "    for col in df_filtered.columns:\n",
    "        count = df_filtered[col].value_counts().iloc[0]\n",
    "        if count / n >= 0.70:\n",
    "                low_variance.append(col)\n",
    "    df_filtered = df_filtered.drop(columns = low_variance)            \n",
    "\n",
    "\n",
    "    #filtering columns with high correlations (no new information)\n",
    "    corr_matrix = df_filtered.corr().abs()\n",
    "    drop = []\n",
    "    all_cols = corr_matrix.columns.tolist()\n",
    "    for row in range(len(all_cols)):\n",
    "        for column in range (row):\n",
    "                if corr_matrix.iloc[row, column] >= 0.80:\n",
    "                    drop.append(all_cols[row])\n",
    "                    break\n",
    "    drop_fin = sorted(set(drop))  \n",
    "\n",
    "\n",
    "    #prepare x and y  \n",
    "    x = df_filtered.drop(columns = drop_fin)\n",
    "    real_fake = pd.read_csv(csv_path_labels)\n",
    "    y = pd.Series(real_fake[labelname], name = labelname)\n",
    "  \n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grid_search_rf(x: pd.DataFrame, y: pd.Series, seed: int): \n",
    "\n",
    "    \"\"\" \n",
    "        param optimazation with grid search\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "\n",
    "            \"n_estimators\": [100, 110, 120, 130, 140, 150, 160, 170, 180, 190],\n",
    "            \"max_depth\": [None, 5, 10, 15, 20],\n",
    "            \"min_samples_leaf\": [2, 3, 4, 5, 6],\n",
    "            \"max_features\": [\"sqrt\", 0.3, 0.5]\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "    #finding the best params for our temp random forrest (accr. metric to beat the score)\n",
    "    rf = RandomForestClassifier(random_state = seed, n_jobs = -1)\n",
    "    opt_grid = GridSearchCV(rf, params, cv = StratifiedKFold(n_splits = 10,\n",
    "                            shuffle = True, random_state = seed), scoring = \"accuracy\",\n",
    "                            n_jobs = -1)\n",
    "    opt_grid.fit(x, y)\n",
    "\n",
    "\n",
    "    return opt_grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_filter (x, y, seed):\n",
    "\n",
    "    \"\"\"\n",
    "        80/20 split and preparation for the final model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # spliting the dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size = 0.2, stratify = y, random_state = seed)\n",
    "    \n",
    "\n",
    "    #extracting params\n",
    "    rf_temp = grid_search_rf(x_train, y_train, seed)\n",
    "\n",
    "\n",
    "    #boruta selection of features \n",
    "    boruta = BorutaPy(estimator = rf_temp, n_estimators = \"auto\", perc = 90, alpha = 0.05, \n",
    "                      max_iter = 70, random_state = seed, verbose = 0)\n",
    "    boruta.fit(x_train, y_train)\n",
    "    selected = [name for name, keep in zip(x_train.columns, boruta.support_) if keep]\n",
    "\n",
    "\n",
    "    #reduction of the subsets\n",
    "    x_train_sel = x_train[selected]\n",
    "    x_test_sel = x_test[selected]\n",
    "\n",
    "\n",
    "    return x_train_sel, x_test_sel, y_train, y_test, selected \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def final_step (x_train_sel, x_test_sel, y_train, y_test, seed):\n",
    "\n",
    "    \"\"\" \n",
    "        Building the last model and report\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rf_final = grid_search_rf(x_train_sel, y_train, seed = seed)\n",
    "    final_model = rf_final.fit(x_train_sel, y_train)\n",
    "    y_pred = final_model.predict(x_test_sel)\n",
    "    report = classification_report(y_test, y_pred, output_dict = True)\n",
    "\n",
    "\n",
    "    return final_model, report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efdd81d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features: ['adj_perc', 'noun_perc', 'pron_perc', 'propn_perc', 'sconj_perc', 'verb_perc', 'anticipation_perc', 'hl_det_perc', 'hl_noun_perc', 'hl_punct_perc', 'hl_verb_perc', 'hl_auto_readability_index']\n",
      "Features len: 12\n",
      "Test performance: {'no': {'precision': 0.875, 'recall': 1.0, 'f1-score': 0.9333333333333333, 'support': 14.0}, 'yes': {'precision': 1.0, 'recall': 0.875, 'f1-score': 0.9333333333333333, 'support': 16.0}, 'accuracy': 0.9333333333333333, 'macro avg': {'precision': 0.9375, 'recall': 0.9375, 'f1-score': 0.9333333333333333, 'support': 30.0}, 'weighted avg': {'precision': 0.9416666666666667, 'recall': 0.9333333333333333, 'f1-score': 0.9333333333333333, 'support': 30.0}}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"output_features.csv\")\n",
    "csv_path_labels = \"group58_stage1.csv\"\n",
    "label = \"real_news\"\n",
    "seed = 37\n",
    "\n",
    "\n",
    "#preparing the data\n",
    "x, y = preparation(df, csv_path_labels, label)\n",
    "\n",
    "\n",
    "#spliting the data in 80/20 and optimize params + feat\n",
    "x_train_sel, x_test_sel, y_train, y_test, features = data_filter(x, y, seed = seed)\n",
    "\n",
    "\n",
    "#building the final model and report\n",
    "final_model, test_report = final_step(x_train_sel, x_test_sel, y_train, y_test, seed = seed)\n",
    "\n",
    "\n",
    "print(f\"Final features: {features}\")\n",
    "print(f\"Features len: {len(features)}\")\n",
    "print(f\"Test performance: {test_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ead87aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "#creating and predicting the final scores\n",
    "x_vars = df[features]\n",
    "y_pred = final_model.predict(x_vars)\n",
    "df_all = x_vars.copy()\n",
    "df_all[\"real_news\"] = y.to_list()\n",
    "df_all[\"real_news_pred\"] = y_pred\n",
    "\n",
    "\n",
    "print(accuracy_score(df_all[\"real_news\"], df_all[\"real_news_pred\"]))\n",
    "output_path = \"all_predictions.csv\"\n",
    "df_all.to_csv(output_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5efc0ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = mean: 0.932, sd: 0.029\n",
      "Recall = mean: 0.934, sd: 0.028\n",
      "f1_score = mean: 0.931, sd: 0.029\n"
     ]
    }
   ],
   "source": [
    "#using the same params and features over 1000 seeds between 50 and 1000\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "random_seeds = random.sample(range(50, 10001), 1000)\n",
    "\n",
    "rf_params = final_model.get_params()\n",
    "rf_params.pop(\"random_state\")\n",
    "\n",
    "accuracies = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for s in random_seeds:\n",
    "    rf = RandomForestClassifier(**rf_params, random_state = s)\n",
    "    rf.fit(x_train_sel, y_train)\n",
    "    y_pred = rf.predict(x_test_sel)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict = True)\n",
    "\n",
    "    accuracies.append(report[\"accuracy\"])\n",
    "    recalls.append(report[\"macro avg\"][\"recall\"])\n",
    "    f1s.append(report[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "\n",
    "print(f\"Accuracy = mean: {np.mean(accuracies):.3f}, sd: {np.std(accuracies):.3f}\")\n",
    "print(f\"Recall = mean: {np.mean(recalls):.3f}, sd: {np.std(recalls):.3f}\")\n",
    "print(f\"f1_score = mean: {np.mean(f1s):.3f}, sd: {np.std(f1s):.3f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
